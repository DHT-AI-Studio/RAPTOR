# Inference API åˆ†æå ±å‘Š

**åˆ†ææ—¥æœŸ**: 2025-10-13  
**åˆ†æè€…**: GitHub Copilot  
**ç‰ˆæœ¬**: v2.0.0

## åŸ·è¡Œæ‘˜è¦

æœ¬å ±å‘Šåˆ†æäº†ç•¶å‰ Inference æ¨¡çµ„èˆ‡å°æ‡‰çš„ API ç«¯é»ä¹‹é–“çš„ä¸€è‡´æ€§ã€‚æ•´é«”è€Œè¨€ï¼ŒAPI å¯¦ç¾å·²ç¶“èˆ‡é‡æ§‹å¾Œçš„ Inference æ¨¡çµ„ä¿æŒè‰¯å¥½çš„å°é½Šï¼Œä½†ä»æœ‰ä¸€äº›æ”¹é€²ç©ºé–“ã€‚

### ç¸½é«”è©•åˆ†ï¼šâ­â­â­â­ (4/5)

**å„ªé»**:
- âœ… API å·²æ­£ç¢ºæ•´åˆæ–°çš„ `inference_manager`
- âœ… æä¾›çµ±ä¸€çš„ `/infer` ç«¯é»
- âœ… å¯¦ç¾äº†å¥åº·æª¢æŸ¥ã€çµ±è¨ˆä¿¡æ¯ç­‰ç®¡ç†ç«¯é»
- âœ… åŒ…å«å‘ä¸‹å…¼å®¹ç«¯é»
- âœ… æä¾›ä½¿ç”¨ç¤ºä¾‹ç«¯é»

**éœ€è¦æ”¹é€²**:
- âš ï¸ ç¼ºå°‘æŸäº›ç·©å­˜ç®¡ç†åŠŸèƒ½çš„å®Œæ•´æš´éœ²
- âš ï¸ æŸäº›éŸ¿æ‡‰æ¨¡å‹å¯ä»¥æ›´è©³ç´°
- âš ï¸ éŒ¯èª¤è™•ç†å¯ä»¥æ›´ç´°ç·»
- âš ï¸ ç¼ºå°‘æ‰¹æ¬¡æ¨ç†ç«¯é»

---

## è©³ç´°åˆ†æ

### 1. æ ¸å¿ƒåŠŸèƒ½å°é½Šåˆ†æ

#### 1.1 æ¨ç†ç®¡ç†å™¨å…¬é–‹æ–¹æ³•

| æ–¹æ³•å | ç”¨é€” | API ç«¯é» | ç‹€æ…‹ |
|-------|------|---------|------|
| `infer()` | åŸ·è¡Œæ¨ç† | `POST /inference/infer` | âœ… å·²å¯¦ç¾ |
| `get_supported_tasks()` | ç²å–æ”¯æŒçš„ä»»å‹™ | `GET /inference/supported-tasks` | âœ… å·²å¯¦ç¾ |
| `get_stats()` | ç²å–çµ±è¨ˆä¿¡æ¯ | `GET /inference/stats` | âœ… å·²å¯¦ç¾ |
| `clear_cache()` | æ¸…ç†ç·©å­˜ | `POST /inference/clear-cache` | âœ… å·²å¯¦ç¾ |
| `health_check()` | å¥åº·æª¢æŸ¥ | `GET /inference/health` | âœ… å·²å¯¦ç¾ |

**çµè«–**: æ‰€æœ‰æ ¸å¿ƒæ–¹æ³•éƒ½å·²æ­£ç¢ºæ˜ å°„åˆ° API ç«¯é»ã€‚

#### 1.2 ç·©å­˜ç®¡ç†å™¨åŠŸèƒ½

| åŠŸèƒ½ | æ–¹æ³• | API ç«¯é» | ç‹€æ…‹ |
|-----|------|---------|------|
| ç²å–ç·©å­˜çµ±è¨ˆ | `cache.get_stats()` | âŒ ç„¡ | âš ï¸ å»ºè­°æ·»åŠ  |
| ç²å–ç·©å­˜æ¨¡å‹åˆ—è¡¨ | `cache.get_cached_models()` | âœ… åœ¨ `/stats` ä¸­ | âœ… å·²åŒ…å« |
| ç§»é™¤ç‰¹å®šæ¨¡å‹ | `cache.remove()` | âŒ ç„¡ | âš ï¸ å»ºè­°æ·»åŠ  |
| èª¿æ•´ç·©å­˜å¤§å° | `cache.resize_cache()` | âŒ ç„¡ | âš ï¸ å»ºè­°æ·»åŠ  |
| è¨­ç½®å…§å­˜é™åˆ¶ | `cache.set_memory_limit()` | âŒ ç„¡ | âš ï¸ å»ºè­°æ·»åŠ  |

**çµè«–**: åŸºæœ¬ç·©å­˜åŠŸèƒ½å·²å¯¦ç¾ï¼Œä½†é€²éšç®¡ç†åŠŸèƒ½ç¼ºå¤±ã€‚

### 2. API ç«¯é»å®Œæ•´æ€§æª¢æŸ¥

#### 2.1 ç¾æœ‰ç«¯é»åˆ—è¡¨

| ç«¯é» | æ–¹æ³• | åŠŸèƒ½ | å„ªå…ˆç´š |
|-----|------|------|-------|
| `/inference/infer` | POST | çµ±ä¸€æ¨ç†æ¥å£ | ğŸ”´ æ ¸å¿ƒ |
| `/inference/health` | GET | å¥åº·æª¢æŸ¥ | ğŸ”´ æ ¸å¿ƒ |
| `/inference/supported-tasks` | GET | æ”¯æŒçš„ä»»å‹™ | ğŸŸ¡ é‡è¦ |
| `/inference/stats` | GET | çµ±è¨ˆä¿¡æ¯ | ğŸŸ¡ é‡è¦ |
| `/inference/clear-cache` | POST | æ¸…ç†ç·©å­˜ | ğŸŸ¡ é‡è¦ |
| `/inference/infer_fixed` | POST | å‘ä¸‹å…¼å®¹ï¼ˆå›ºå®šæ¨¡å‹ï¼‰ | ğŸŸ¢ å¯é¸ |
| `/inference/infer_multimodal` | POST | å‘ä¸‹å…¼å®¹ï¼ˆå¤šæ¨¡æ…‹ï¼‰ | ğŸŸ¢ å¯é¸ |
| `/inference/examples` | GET | ä½¿ç”¨ç¤ºä¾‹ | ğŸŸ¢ å¯é¸ |

#### 2.2 å»ºè­°æ–°å¢ç«¯é»

| å»ºè­°ç«¯é» | æ–¹æ³• | åŠŸèƒ½ | å„ªå…ˆç´š | ç†ç”± |
|---------|------|------|-------|------|
| `/inference/cache/stats` | GET | è©³ç´°ç·©å­˜çµ±è¨ˆ | ğŸŸ¡ é‡è¦ | æä¾›æ›´ç´°ç·»çš„ç·©å­˜ç›£æ§ |
| `/inference/cache/models` | GET | ç·©å­˜æ¨¡å‹åˆ—è¡¨ | ğŸŸ¡ é‡è¦ | ç®¡ç†ç·©å­˜æ¨¡å‹ |
| `/inference/cache/remove/{model_key}` | DELETE | ç§»é™¤ç‰¹å®šæ¨¡å‹ | ğŸŸ¢ å¯é¸ | ç²¾ç´°åŒ–ç·©å­˜ç®¡ç† |
| `/inference/cache/config` | PUT | æ›´æ–°ç·©å­˜é…ç½® | ğŸŸ¢ å¯é¸ | å‹•æ…‹èª¿æ•´ç·©å­˜åƒæ•¸ |
| `/inference/batch` | POST | æ‰¹æ¬¡æ¨ç† | ğŸŸ¡ é‡è¦ | æé«˜æ•ˆç‡ |
| `/inference/async` | POST | ç•°æ­¥æ¨ç† | ğŸŸ¢ å¯é¸ | é•·æ™‚é–“æ¨ç†ä»»å‹™ |
| `/inference/models/preload` | POST | é åŠ è¼‰æ¨¡å‹ | ğŸŸ¢ å¯é¸ | å„ªåŒ–é¦–æ¬¡æ¨ç†æ™‚é–“ |

### 3. è«‹æ±‚/éŸ¿æ‡‰æ¨¡å‹åˆ†æ

#### 3.1 ç¾æœ‰æ¨¡å‹

```python
âœ… InferenceRequest - å®Œæ•´ä¸”æ­£ç¢º
âœ… HealthCheckResponse - é©ç•¶
âœ… SupportedTasksResponse - é©ç•¶
âœ… StatsResponse - åŸºæœ¬è¶³å¤ 
```

#### 3.2 å»ºè­°æ”¹é€²

```python
class InferenceResponse(BaseModel):
    """å»ºè­°ï¼šçµ±ä¸€çš„æ¨ç†éŸ¿æ‡‰æ¨¡å‹"""
    success: bool
    result: Optional[Dict[str, Any]]
    error: Optional[str]
    task: str
    engine: str
    model_name: str
    processing_time: float
    timestamp: float
    request_id: Optional[str]
    metadata: Optional[Dict[str, Any]]

class CacheStatsResponse(BaseModel):
    """å»ºè­°ï¼šè©³ç´°çš„ç·©å­˜çµ±è¨ˆéŸ¿æ‡‰"""
    cache_size: int
    max_cache_size: int
    hit_rate: float
    miss_rate: float
    eviction_rate: float
    total_memory_mb: float
    max_memory_mb: float
    models: List[CachedModelInfo]

class BatchInferenceRequest(BaseModel):
    """å»ºè­°ï¼šæ‰¹æ¬¡æ¨ç†è«‹æ±‚"""
    requests: List[InferenceRequest]
    parallel: bool = False
    max_workers: Optional[int] = None
```

### 4. éŒ¯èª¤è™•ç†åˆ†æ

#### 4.1 ç¾æœ‰éŒ¯èª¤è™•ç†

```python
âœ… ValueError â†’ 400 Bad Request
âœ… Exception â†’ 500 Internal Server Error
```

#### 4.2 å»ºè­°æ”¹é€²

```python
# å»ºè­°å®šç¾©æ›´ç´°ç·»çš„ç•°å¸¸æ˜ å°„
exception_status_map = {
    UnsupportedTaskError: 400,
    ModelNotFoundError: 404,
    InferenceError: 500,
    TimeoutError: 504,
    ResourceExhaustedError: 503
}

# å»ºè­°çµ±ä¸€çš„éŒ¯èª¤éŸ¿æ‡‰æ ¼å¼
{
    "success": false,
    "error": {
        "type": "ModelNotFoundError",
        "message": "æ¨¡å‹ 'xxx' æœªåœ¨ MLflow ä¸­æ‰¾åˆ°",
        "code": "MODEL_NOT_FOUND",
        "details": {...}
    },
    "timestamp": 1234567890.123,
    "request_id": "abc123"
}
```

### 5. æ€§èƒ½èˆ‡å®‰å…¨è€ƒæ…®

#### 5.1 æ€§èƒ½å„ªåŒ–å»ºè­°

| å„ªåŒ–é … | ç•¶å‰ç‹€æ…‹ | å»ºè­°æ”¹é€² | å„ªå…ˆç´š |
|-------|---------|---------|-------|
| è«‹æ±‚é©—è­‰ | Pydantic è‡ªå‹•é©—è­‰ | âœ… å·²å„ªåŒ– | - |
| éŸ¿æ‡‰å£“ç¸® | æœªå¯¦ç¾ | æ·»åŠ  gzip ä¸­é–“ä»¶ | ğŸŸ¢ å¯é¸ |
| è«‹æ±‚é™æµ | æœªå¯¦ç¾ | æ·»åŠ  rate limiting | ğŸŸ¡ é‡è¦ |
| è«‹æ±‚è¿½è¹¤ | ç°¡å–®çš„ request_id | å®Œæ•´çš„åˆ†å¸ƒå¼è¿½è¹¤ | ğŸŸ¢ å¯é¸ |
| ç•°æ­¥è™•ç† | åŒæ­¥é˜»å¡ | æ”¯æŒç•°æ­¥æ¨ç† | ğŸŸ¡ é‡è¦ |

#### 5.2 å®‰å…¨æ€§å»ºè­°

| å®‰å…¨é … | ç•¶å‰ç‹€æ…‹ | å»ºè­°æ”¹é€² | å„ªå…ˆç´š |
|-------|---------|---------|-------|
| èªè­‰ | æœªå¯¦ç¾ | API Key / JWT | ğŸ”´ æ ¸å¿ƒ |
| æˆæ¬Š | æœªå¯¦ç¾ | RBAC | ğŸŸ¡ é‡è¦ |
| è¼¸å…¥é©—è­‰ | Pydantic é©—è­‰ | âœ… å·²å¯¦ç¾ | - |
| è¼¸å‡ºéæ¿¾ | ç„¡ | æ•æ„Ÿä¿¡æ¯éæ¿¾ | ğŸŸ¡ é‡è¦ |
| CORS | æœªé…ç½® | é…ç½® CORS ç­–ç•¥ | ğŸŸ¡ é‡è¦ |
| SSL/TLS | ä¾è³´éƒ¨ç½² | å¼·åˆ¶ HTTPS | ğŸ”´ æ ¸å¿ƒ |

### 6. æ–‡æª”èˆ‡å¯ç”¨æ€§

#### 6.1 æ–‡æª”å®Œæ•´æ€§

```
âœ… API ç«¯é»æœ‰è©³ç´°çš„ docstring
âœ… åŒ…å«ä½¿ç”¨ç¯„ä¾‹
âœ… æä¾› /examples ç«¯é»
âœ… FastAPI è‡ªå‹•ç”Ÿæˆ OpenAPI æ–‡æª”
âš ï¸ ç¼ºå°‘ç¨ç«‹çš„ API æ–‡æª”ï¼ˆå¦‚ API_GUIDE.mdï¼‰
```

#### 6.2 å»ºè­°æ·»åŠ 

1. **API æ–‡æª”**: å‰µå»º `src/api/API_REFERENCE.md`
2. **éŒ¯èª¤ç¢¼æ–‡æª”**: å‰µå»º `src/api/ERROR_CODES.md`
3. **é·ç§»æŒ‡å—**: å‰µå»º `src/api/MIGRATION_GUIDE.md`ï¼ˆå¦‚æœæœ‰èˆŠç‰ˆæœ¬ï¼‰

---

## æ¨è–¦çš„æ›´æ–°å„ªå…ˆç´š

### ğŸ”´ é«˜å„ªå…ˆç´šï¼ˆç«‹å³å¯¦æ–½ï¼‰

1. **æ·»åŠ èªè­‰æ©Ÿåˆ¶**
   - API Key èªè­‰
   - åŸºæœ¬çš„è«‹æ±‚é™æµ

2. **æ”¹é€²éŒ¯èª¤è™•ç†**
   - çµ±ä¸€çš„éŒ¯èª¤éŸ¿æ‡‰æ ¼å¼
   - æ›´ç´°ç·»çš„ç•°å¸¸æ˜ å°„

3. **æ·»åŠ æ‰¹æ¬¡æ¨ç†ç«¯é»**
   ```python
   @router.post("/batch")
   def batch_inference(requests: List[InferenceRequest])
   ```

### ğŸŸ¡ ä¸­å„ªå…ˆç´šï¼ˆè¿‘æœŸå¯¦æ–½ï¼‰

4. **æ“´å±•ç·©å­˜ç®¡ç† API**
   ```python
   @router.get("/cache/stats")
   @router.delete("/cache/models/{model_key}")
   @router.put("/cache/config")
   ```

5. **æ·»åŠ ç•°æ­¥æ¨ç†æ”¯æŒ**
   ```python
   @router.post("/async")
   async def async_inference(request: InferenceRequest)
   ```

6. **å®Œå–„éŸ¿æ‡‰æ¨¡å‹**
   - å®šç¾©çµ±ä¸€çš„ `InferenceResponse`
   - æ·»åŠ æ›´å¤šå…ƒæ•¸æ“š

### ğŸŸ¢ ä½å„ªå…ˆç´šï¼ˆæœªä¾†è€ƒæ…®ï¼‰

7. **æ¨¡å‹é åŠ è¼‰ç«¯é»**
   ```python
   @router.post("/models/preload")
   def preload_model(model_name: str, engine: str)
   ```

8. **WebSocket æ”¯æŒ**
   - ç”¨æ–¼æµå¼æ¨ç†

9. **ç›£æ§å„€è¡¨æ¿**
   - å¯¦æ™‚çµ±è¨ˆå¯è¦–åŒ–

---

## å…·é«”ä»£ç¢¼å»ºè­°

### å»ºè­° 1: æ·»åŠ è©³ç´°ç·©å­˜ç®¡ç†ç«¯é»

```python
@router.get("/cache/stats", summary="ç²å–è©³ç´°ç·©å­˜çµ±è¨ˆ")
def get_cache_stats():
    """
    ç²å–è©³ç´°çš„ç·©å­˜çµ±è¨ˆä¿¡æ¯
    
    åŒ…æ‹¬ç·©å­˜å‘½ä¸­ç‡ã€å…§å­˜ä½¿ç”¨ã€æ¨¡å‹åˆ—è¡¨ç­‰ã€‚
    """
    try:
        cache_stats = inference_manager.cache.get_stats()
        return {
            "success": True,
            "stats": cache_stats,
            "timestamp": time.time()
        }
    except Exception as e:
        logger.error(f"ç²å–ç·©å­˜çµ±è¨ˆå¤±æ•—: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç²å–ç·©å­˜çµ±è¨ˆå¤±æ•—: {str(e)}"
        )

@router.get("/cache/models", summary="ç²å–ç·©å­˜æ¨¡å‹åˆ—è¡¨")
def get_cached_models():
    """
    ç²å–ç•¶å‰ç·©å­˜ä¸­çš„æ‰€æœ‰æ¨¡å‹
    
    è¿”å›æ¨¡å‹éµå€¼ã€å…ƒæ•¸æ“šã€è¨ªå•çµ±è¨ˆç­‰ã€‚
    """
    try:
        cached_models = inference_manager.cache.get_cached_models()
        return {
            "success": True,
            "models": cached_models,
            "count": len(cached_models),
            "timestamp": time.time()
        }
    except Exception as e:
        logger.error(f"ç²å–ç·©å­˜æ¨¡å‹åˆ—è¡¨å¤±æ•—: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç²å–ç·©å­˜æ¨¡å‹åˆ—è¡¨å¤±æ•—: {str(e)}"
        )

@router.delete("/cache/models/{model_key}", summary="ç§»é™¤ç‰¹å®šç·©å­˜æ¨¡å‹")
def remove_cached_model(model_key: str):
    """
    å¾ç·©å­˜ä¸­ç§»é™¤ç‰¹å®šæ¨¡å‹
    
    Args:
        model_key: æ¨¡å‹éµå€¼ï¼ˆæ ¼å¼ï¼šengine:model_nameï¼‰
    """
    try:
        success = inference_manager.cache.remove(model_key)
        return {
            "success": success,
            "message": f"æ¨¡å‹ {model_key} å·²å¾ç·©å­˜ç§»é™¤" if success else f"æ¨¡å‹ {model_key} ä¸åœ¨ç·©å­˜ä¸­",
            "timestamp": time.time()
        }
    except Exception as e:
        logger.error(f"ç§»é™¤ç·©å­˜æ¨¡å‹å¤±æ•—: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ç§»é™¤ç·©å­˜æ¨¡å‹å¤±æ•—: {str(e)}"
        )

@router.put("/cache/config", summary="æ›´æ–°ç·©å­˜é…ç½®")
def update_cache_config(
    max_cache_size: Optional[int] = None,
    max_memory_gb: Optional[float] = None
):
    """
    å‹•æ…‹æ›´æ–°ç·©å­˜é…ç½®
    
    Args:
        max_cache_size: æœ€å¤§ç·©å­˜æ¨¡å‹æ•¸é‡
        max_memory_gb: æœ€å¤§å…§å­˜ä½¿ç”¨ï¼ˆGBï¼‰
    """
    try:
        if max_cache_size is not None:
            inference_manager.cache.resize_cache(max_cache_size)
        
        if max_memory_gb is not None:
            inference_manager.cache.set_memory_limit(max_memory_gb)
        
        return {
            "success": True,
            "message": "ç·©å­˜é…ç½®å·²æ›´æ–°",
            "config": {
                "max_cache_size": inference_manager.cache.max_cache_size,
                "max_memory_gb": inference_manager.cache.max_memory_bytes / (1024**3)
            },
            "timestamp": time.time()
        }
    except Exception as e:
        logger.error(f"æ›´æ–°ç·©å­˜é…ç½®å¤±æ•—: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ›´æ–°ç·©å­˜é…ç½®å¤±æ•—: {str(e)}"
        )
```

### å»ºè­° 2: æ·»åŠ æ‰¹æ¬¡æ¨ç†ç«¯é»

```python
class BatchInferenceRequest(BaseModel):
    """æ‰¹æ¬¡æ¨ç†è«‹æ±‚æ¨¡å‹"""
    requests: List[InferenceRequest] = Field(
        description="æ¨ç†è«‹æ±‚åˆ—è¡¨",
        min_items=1,
        max_items=100  # é™åˆ¶æ‰¹æ¬¡å¤§å°
    )
    parallel: bool = Field(
        default=False,
        description="æ˜¯å¦ä¸¦è¡ŒåŸ·è¡Œï¼ˆéœ€è¦è¶³å¤ çš„è³‡æºï¼‰"
    )

@router.post("/batch", summary="æ‰¹æ¬¡æ¨ç†")
def batch_inference(batch_request: BatchInferenceRequest):
    """
    æ‰¹æ¬¡æ¨ç†æ¥å£
    
    ä¸€æ¬¡æ€§æäº¤å¤šå€‹æ¨ç†è«‹æ±‚ï¼Œå¯é¸æ“‡ä¸¦è¡Œæˆ–é †åºåŸ·è¡Œã€‚
    
    **é™åˆ¶**:
    - æœ€å¤š 100 å€‹è«‹æ±‚/æ‰¹æ¬¡
    - ä¸¦è¡Œæ¨¡å¼éœ€è¦è¶³å¤ çš„ GPU/CPU è³‡æº
    
    **å»ºè­°**:
    - ç›¸åŒæ¨¡å‹çš„è«‹æ±‚åˆ†çµ„å¯æé«˜æ•ˆç‡
    - å¤§æ‰¹æ¬¡å»ºè­°ä½¿ç”¨é †åºæ¨¡å¼é¿å…è³‡æºè€—ç›¡
    """
    try:
        start_time = time.time()
        results = []
        
        if batch_request.parallel:
            # ä¸¦è¡ŒåŸ·è¡Œï¼ˆéœ€è¦å¯¦ç¾ç·šç¨‹æ± /é€²ç¨‹æ± ï¼‰
            # TODO: å¯¦ç¾ä¸¦è¡Œæ¨ç†é‚è¼¯
            logger.warning("ä¸¦è¡Œæ¨¡å¼å°šæœªå®Œå…¨å¯¦ç¾ï¼Œå°‡ä½¿ç”¨é †åºæ¨¡å¼")
        
        # é †åºåŸ·è¡Œ
        for idx, req in enumerate(batch_request.requests):
            try:
                result = inference_manager.infer(
                    task=req.task,
                    engine=req.engine,
                    model_name=req.model_name,
                    data=req.data,
                    options=req.options
                )
                results.append({
                    "index": idx,
                    "success": True,
                    "result": result
                })
            except Exception as e:
                results.append({
                    "index": idx,
                    "success": False,
                    "error": str(e)
                })
        
        total_time = time.time() - start_time
        success_count = sum(1 for r in results if r['success'])
        
        return {
            "success": True,
            "total_requests": len(batch_request.requests),
            "successful": success_count,
            "failed": len(batch_request.requests) - success_count,
            "results": results,
            "total_processing_time": total_time,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡æ¨ç†å¤±æ•—: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ‰¹æ¬¡æ¨ç†å¤±æ•—: {str(e)}"
        )
```

### å»ºè­° 3: æ”¹é€²éŒ¯èª¤è™•ç†

```python
# åœ¨æ–‡ä»¶é ‚éƒ¨æ·»åŠ 
from ..inference.manager import InferenceError, ModelNotFoundError, UnsupportedTaskError

# æ·»åŠ è‡ªå®šç¾©ç•°å¸¸è™•ç†å™¨
@router.exception_handler(UnsupportedTaskError)
async def unsupported_task_handler(request, exc):
    return JSONResponse(
        status_code=400,
        content={
            "success": False,
            "error": {
                "type": "UnsupportedTaskError",
                "message": str(exc),
                "code": "UNSUPPORTED_TASK"
            },
            "timestamp": time.time()
        }
    )

@router.exception_handler(ModelNotFoundError)
async def model_not_found_handler(request, exc):
    return JSONResponse(
        status_code=404,
        content={
            "success": False,
            "error": {
                "type": "ModelNotFoundError",
                "message": str(exc),
                "code": "MODEL_NOT_FOUND"
            },
            "timestamp": time.time()
        }
    )
```

---

## çµè«–

ç•¶å‰çš„ Inference API å¯¦ç¾å·²ç¶“èˆ‡é‡æ§‹å¾Œçš„æ¨¡çµ„ä¿æŒè‰¯å¥½å°é½Šï¼ŒåŸºæœ¬åŠŸèƒ½å®Œæ•´ä¸”æ­£ç¢ºã€‚ä¸»è¦çš„æ”¹é€²ç©ºé–“åœ¨æ–¼ï¼š

1. **æ“´å±•ç·©å­˜ç®¡ç†åŠŸèƒ½** - æä¾›æ›´ç´°ç·»çš„ç·©å­˜æ§åˆ¶
2. **æ·»åŠ æ‰¹æ¬¡æ¨ç†** - æé«˜æ•ˆç‡å’Œååé‡
3. **å®Œå–„éŒ¯èª¤è™•ç†** - æ›´å‹å¥½çš„éŒ¯èª¤ä¿¡æ¯
4. **å¢å¼·å®‰å…¨æ€§** - èªè­‰å’Œæˆæ¬Šæ©Ÿåˆ¶
5. **æ€§èƒ½å„ªåŒ–** - ç•°æ­¥è™•ç†ã€è«‹æ±‚é™æµ

é€™äº›æ”¹é€²å¯ä»¥æ ¹æ“šå„ªå…ˆç´šé€æ­¥å¯¦æ–½ï¼Œä¸æœƒå½±éŸ¿ç¾æœ‰åŠŸèƒ½çš„ç©©å®šæ€§ã€‚

---

## ä¸‹ä¸€æ­¥è¡Œå‹•

### ç«‹å³è¡Œå‹•ï¼ˆæœ¬å‘¨å…§ï¼‰
- [ ] æ·»åŠ ç·©å­˜ç®¡ç† API ç«¯é»
- [ ] å¯¦ç¾æ‰¹æ¬¡æ¨ç†åŠŸèƒ½
- [ ] æ”¹é€²éŒ¯èª¤è™•ç†æ©Ÿåˆ¶

### çŸ­æœŸè¡Œå‹•ï¼ˆæœ¬æœˆå…§ï¼‰
- [ ] æ·»åŠ  API èªè­‰
- [ ] å¯¦ç¾è«‹æ±‚é™æµ
- [ ] å‰µå»ºå®Œæ•´çš„ API æ–‡æª”

### é•·æœŸè¡Œå‹•ï¼ˆä¸‹å­£åº¦ï¼‰
- [ ] ç•°æ­¥æ¨ç†æ”¯æŒ
- [ ] åˆ†å¸ƒå¼è¿½è¹¤
- [ ] ç›£æ§å„€è¡¨æ¿

---

**å ±å‘Šç”Ÿæˆæ™‚é–“**: 2025-10-13  
**å¯©æŸ¥å»ºè­°**: å»ºè­°ç”±æŠ€è¡“è² è²¬äººå¯©æŸ¥ä¸¦æ±ºå®šå¯¦æ–½å„ªå…ˆç´š
