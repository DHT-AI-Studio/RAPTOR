# Inference API æ›´æ–°å»ºè­°

**æ—¥æœŸ**: 2025-10-13  
**ç›®æ¨™**: æ ¹æ“šé‡æ§‹å¾Œçš„ Inference æ¨¡çµ„æ›´æ–° API

---

## åŸ·è¡Œæ‘˜è¦

âœ… **å¥½æ¶ˆæ¯**: ç•¶å‰ API å·²ç¶“æ­£ç¢ºæ•´åˆäº†é‡æ§‹å¾Œçš„ inference_managerï¼Œæ ¸å¿ƒåŠŸèƒ½å®Œæ•´ã€‚

âš ï¸ **æ”¹é€²ç©ºé–“**: å»ºè­°æ·»åŠ ä¸€äº›é€²éšåŠŸèƒ½ä»¥æå‡ API çš„å®Œæ•´æ€§å’Œå¯ç”¨æ€§ã€‚

---

## å¿«é€Ÿæª¢æŸ¥çµæœ

### âœ… å·²æ­£ç¢ºå¯¦ç¾çš„åŠŸèƒ½

1. **çµ±ä¸€æ¨ç†ç«¯é»** (`POST /inference/infer`)
   - âœ… æ­£ç¢ºèª¿ç”¨ `inference_manager.infer()`
   - âœ… æ”¯æŒæ‰€æœ‰ä»»å‹™é¡å‹
   - âœ… å®Œæ•´çš„åƒæ•¸é©—è­‰
   - âœ… è©³ç´°çš„æ–‡æª”

2. **å¥åº·æª¢æŸ¥** (`GET /inference/health`)
   - âœ… èª¿ç”¨ `inference_manager.health_check()`
   - âœ… è¿”å›çµ„ä»¶ç‹€æ…‹

3. **ä»»å‹™åˆ—è¡¨** (`GET /inference/supported-tasks`)
   - âœ… èª¿ç”¨ `inference_manager.get_supported_tasks()`

4. **çµ±è¨ˆä¿¡æ¯** (`GET /inference/stats`)
   - âœ… èª¿ç”¨ `inference_manager.get_stats()`

5. **æ¸…ç†ç·©å­˜** (`POST /inference/clear-cache`)
   - âœ… èª¿ç”¨ `inference_manager.clear_cache()`

6. **å‘ä¸‹å…¼å®¹** (`/infer_fixed`, `/infer_multimodal`)
   - âœ… æä¾›èˆŠç‰ˆç«¯é»å…¼å®¹

7. **ä½¿ç”¨ç¤ºä¾‹** (`GET /inference/examples`)
   - âœ… æä¾›å®Œæ•´ç¤ºä¾‹

### âš ï¸ å»ºè­°æ·»åŠ çš„åŠŸèƒ½

#### é«˜å„ªå…ˆç´š

1. **è©³ç´°ç·©å­˜ç®¡ç†**
   ```
   GET    /inference/cache/stats          - è©³ç´°ç·©å­˜çµ±è¨ˆ
   GET    /inference/cache/models         - ç·©å­˜æ¨¡å‹åˆ—è¡¨
   DELETE /inference/cache/models/{key}   - ç§»é™¤ç‰¹å®šæ¨¡å‹
   PUT    /inference/cache/config         - æ›´æ–°ç·©å­˜é…ç½®
   ```

2. **æ‰¹æ¬¡æ¨ç†**
   ```
   POST /inference/batch - æ‰¹æ¬¡è™•ç†å¤šå€‹è«‹æ±‚
   ```

3. **æ”¹é€²çš„éŒ¯èª¤è™•ç†**
   - çµ±ä¸€éŒ¯èª¤éŸ¿æ‡‰æ ¼å¼
   - è‡ªå®šç¾©ç•°å¸¸è™•ç†å™¨

#### ä¸­å„ªå…ˆç´š

4. **ç•°æ­¥æ¨ç†**
   ```
   POST /inference/async        - æäº¤ç•°æ­¥ä»»å‹™
   GET  /inference/async/{id}   - æŸ¥è©¢ä»»å‹™ç‹€æ…‹
   ```

5. **æ¨¡å‹é åŠ è¼‰**
   ```
   POST /inference/models/preload  - é åŠ è¼‰æ¨¡å‹åˆ°ç·©å­˜
   ```

#### ä½å„ªå…ˆç´š

6. **æµå¼æ¨ç†** (WebSocket)
7. **ç›£æ§å„€è¡¨æ¿**

---

## è©³ç´°å»ºè­°

### å»ºè­° 1: æ·»åŠ ç·©å­˜ç®¡ç†ç«¯é» ğŸ”´ é«˜å„ªå…ˆç´š

**åŸå› **: `ModelCache` é¡æä¾›äº†è±å¯Œçš„æ–¹æ³•ï¼Œä½† API åƒ…æš´éœ²äº†åŸºæœ¬çš„ `clear_cache()`

**å¯ç”¨æ–¹æ³•**:
- `cache.get_stats()` - è©³ç´°çµ±è¨ˆ
- `cache.get_cached_models()` - æ¨¡å‹åˆ—è¡¨
- `cache.remove(model_key)` - ç§»é™¤ç‰¹å®šæ¨¡å‹
- `cache.resize_cache(size)` - èª¿æ•´ç·©å­˜å¤§å°
- `cache.set_memory_limit(gb)` - è¨­ç½®å…§å­˜é™åˆ¶

**å»ºè­°å¯¦ç¾**:

```python
@router.get("/cache/stats", summary="ç²å–è©³ç´°ç·©å­˜çµ±è¨ˆ")
def get_detailed_cache_stats():
    """
    ç²å–è©³ç´°çš„ç·©å­˜çµ±è¨ˆä¿¡æ¯
    
    è¿”å›:
    - ç·©å­˜å‘½ä¸­ç‡
    - å…§å­˜ä½¿ç”¨æƒ…æ³
    - æ¨¡å‹è¨ªå•çµ±è¨ˆ
    - é©…é€çµ±è¨ˆ
    """
    try:
        cache_stats = inference_manager.cache.get_stats()
        return {
            "success": True,
            "stats": cache_stats,
            "timestamp": time.time()
        }
    except Exception as e:
        logger.error(f"ç²å–ç·©å­˜çµ±è¨ˆå¤±æ•—: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ç²å–ç·©å­˜çµ±è¨ˆå¤±æ•—: {str(e)}"
        )

@router.get("/cache/models", summary="ç²å–ç·©å­˜æ¨¡å‹åˆ—è¡¨")
def get_cached_model_list():
    """
    ç²å–ç•¶å‰ç·©å­˜ä¸­çš„æ‰€æœ‰æ¨¡å‹
    
    åŒ…å«æ¯å€‹æ¨¡å‹çš„:
    - æ¨¡å‹éµå€¼
    - åŠ è¼‰æ™‚é–“
    - è¨ªå•æ¬¡æ•¸
    - æœ€å¾Œè¨ªå•æ™‚é–“
    - å…§å­˜å ç”¨
    """
    try:
        cached_models = inference_manager.cache.get_cached_models()
        return {
            "success": True,
            "models": cached_models,
            "count": len(cached_models),
            "timestamp": time.time()
        }
    except Exception as e:
        logger.error(f"ç²å–ç·©å­˜æ¨¡å‹åˆ—è¡¨å¤±æ•—: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ç²å–ç·©å­˜æ¨¡å‹åˆ—è¡¨å¤±æ•—: {str(e)}"
        )

@router.delete("/cache/models/{model_key}", summary="ç§»é™¤ç‰¹å®šç·©å­˜æ¨¡å‹")
def remove_specific_cached_model(model_key: str):
    """
    å¾ç·©å­˜ä¸­ç§»é™¤ç‰¹å®šæ¨¡å‹
    
    Args:
        model_key: æ¨¡å‹éµå€¼ï¼ˆæ ¼å¼ï¼šengine:model_nameï¼‰
        
    ç¤ºä¾‹:
        DELETE /inference/cache/models/ollama:llama2-7b
    """
    try:
        success = inference_manager.cache.remove(model_key)
        if success:
            return {
                "success": True,
                "message": f"æ¨¡å‹ '{model_key}' å·²å¾ç·©å­˜ç§»é™¤",
                "timestamp": time.time()
            }
        else:
            return {
                "success": False,
                "message": f"æ¨¡å‹ '{model_key}' ä¸åœ¨ç·©å­˜ä¸­",
                "timestamp": time.time()
            }
    except Exception as e:
        logger.error(f"ç§»é™¤ç·©å­˜æ¨¡å‹å¤±æ•—: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ç§»é™¤ç·©å­˜æ¨¡å‹å¤±æ•—: {str(e)}"
        )

@router.put("/cache/config", summary="æ›´æ–°ç·©å­˜é…ç½®")
def update_cache_configuration(
    max_cache_size: Optional[int] = Query(None, ge=1, le=20, description="æœ€å¤§ç·©å­˜æ¨¡å‹æ•¸é‡"),
    max_memory_gb: Optional[float] = Query(None, ge=1.0, le=100.0, description="æœ€å¤§å…§å­˜ä½¿ç”¨ï¼ˆGBï¼‰")
):
    """
    å‹•æ…‹æ›´æ–°ç·©å­˜é…ç½®
    
    å¯ä»¥èª¿æ•´:
    - max_cache_size: ç·©å­˜ä¸­ä¿å­˜çš„æœ€å¤§æ¨¡å‹æ•¸é‡
    - max_memory_gb: ç·©å­˜å¯ä½¿ç”¨çš„æœ€å¤§å…§å­˜
    
    æ³¨æ„:
    - æ¸›å°ç·©å­˜å¤§å°å¯èƒ½è§¸ç™¼æ¨¡å‹é©…é€
    - è¨­ç½®æœƒç«‹å³ç”Ÿæ•ˆ
    """
    try:
        updates = []
        
        if max_cache_size is not None:
            inference_manager.cache.resize_cache(max_cache_size)
            updates.append(f"max_cache_size æ›´æ–°ç‚º {max_cache_size}")
        
        if max_memory_gb is not None:
            inference_manager.cache.set_memory_limit(max_memory_gb)
            updates.append(f"max_memory_gb æ›´æ–°ç‚º {max_memory_gb}")
        
        if not updates:
            raise ValueError("è‡³å°‘éœ€è¦æä¾›ä¸€å€‹é…ç½®åƒæ•¸")
        
        return {
            "success": True,
            "message": "ç·©å­˜é…ç½®å·²æ›´æ–°",
            "updates": updates,
            "current_config": {
                "max_cache_size": inference_manager.cache.max_cache_size,
                "max_memory_gb": inference_manager.cache.max_memory_bytes / (1024**3)
            },
            "timestamp": time.time()
        }
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"æ›´æ–°ç·©å­˜é…ç½®å¤±æ•—: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"æ›´æ–°ç·©å­˜é…ç½®å¤±æ•—: {str(e)}"
        )
```

**å½±éŸ¿**: 
- âœ… æä¾›æ›´ç²¾ç´°çš„ç·©å­˜ç®¡ç†
- âœ… æœ‰åŠ©æ–¼æ€§èƒ½èª¿å„ª
- âœ… ä¸å½±éŸ¿ç¾æœ‰åŠŸèƒ½

---

### å»ºè­° 2: æ·»åŠ æ‰¹æ¬¡æ¨ç†ç«¯é» ğŸ”´ é«˜å„ªå…ˆç´š

**åŸå› **: æé«˜ååé‡ï¼Œæ¸›å°‘ç¶²çµ¡é–‹éŠ·

**å»ºè­°å¯¦ç¾**:

```python
from typing import List
from pydantic import Field, validator

class BatchInferenceRequest(BaseModel):
    """æ‰¹æ¬¡æ¨ç†è«‹æ±‚æ¨¡å‹"""
    requests: List[InferenceRequest] = Field(
        ...,
        description="æ¨ç†è«‹æ±‚åˆ—è¡¨",
        min_items=1,
        max_items=100
    )
    parallel: bool = Field(
        default=False,
        description="æ˜¯å¦ä¸¦è¡ŒåŸ·è¡Œï¼ˆéœ€è¦è¶³å¤ è³‡æºï¼‰"
    )
    
    @validator('requests')
    def validate_requests(cls, v):
        if len(v) > 100:
            raise ValueError("æ‰¹æ¬¡å¤§å°ä¸èƒ½è¶…é 100")
        return v

@router.post("/batch", summary="æ‰¹æ¬¡æ¨ç†")
def batch_inference_endpoint(batch_request: BatchInferenceRequest):
    """
    æ‰¹æ¬¡æ¨ç†æ¥å£
    
    ä¸€æ¬¡æ€§æäº¤å¤šå€‹æ¨ç†è«‹æ±‚ï¼Œå¯é¸æ“‡é †åºæˆ–ä¸¦è¡ŒåŸ·è¡Œã€‚
    
    **é™åˆ¶**:
    - æœ€å¤š 100 å€‹è«‹æ±‚/æ‰¹æ¬¡
    - ä¸¦è¡Œæ¨¡å¼éœ€è¦è¶³å¤ çš„è³‡æº
    
    **å»ºè­°**:
    - ç›¸åŒæ¨¡å‹çš„è«‹æ±‚åˆ†çµ„å¯æé«˜ç·©å­˜æ•ˆç‡
    - å¤§æ‰¹æ¬¡å»ºè­°ä½¿ç”¨é †åºæ¨¡å¼é¿å…è³‡æºè€—ç›¡
    
    **ç¤ºä¾‹**:
    ```json
    {
      "requests": [
        {
          "task": "text-generation",
          "engine": "ollama",
          "model_name": "llama2-7b",
          "data": {"inputs": "Hello"},
          "options": {}
        },
        {
          "task": "text-generation",
          "engine": "ollama",
          "model_name": "llama2-7b",
          "data": {"inputs": "World"},
          "options": {}
        }
      ],
      "parallel": false
    }
    ```
    """
    try:
        start_time = time.time()
        results = []
        
        # é †åºåŸ·è¡Œï¼ˆå®‰å…¨ä¸”å¯é æ¸¬ï¼‰
        for idx, req in enumerate(batch_request.requests):
            try:
                result = inference_manager.infer(
                    task=req.task,
                    engine=req.engine,
                    model_name=req.model_name,
                    data=req.data,
                    options=req.options or {}
                )
                results.append({
                    "index": idx,
                    "success": True,
                    "result": result
                })
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡è«‹æ±‚ {idx} å¤±æ•—: {e}")
                results.append({
                    "index": idx,
                    "success": False,
                    "error": str(e),
                    "task": req.task,
                    "model_name": req.model_name
                })
        
        total_time = time.time() - start_time
        success_count = sum(1 for r in results if r.get('success', False))
        failed_count = len(results) - success_count
        
        return {
            "success": True,
            "summary": {
                "total_requests": len(batch_request.requests),
                "successful": success_count,
                "failed": failed_count,
                "success_rate": success_count / len(batch_request.requests) if results else 0
            },
            "results": results,
            "total_processing_time": total_time,
            "average_time_per_request": total_time / len(results) if results else 0,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡æ¨ç†åŸ·è¡Œå¤±æ•—: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"æ‰¹æ¬¡æ¨ç†åŸ·è¡Œå¤±æ•—: {str(e)}"
        )
```

**å½±éŸ¿**:
- âœ… æé«˜æ‰¹é‡è™•ç†æ•ˆç‡
- âœ… æ¸›å°‘ç¶²çµ¡å¾€è¿”æ¬¡æ•¸
- âœ… æ›´å¥½çš„ç·©å­˜åˆ©ç”¨

---

### å»ºè­° 3: æ”¹é€²éŒ¯èª¤è™•ç† ğŸŸ¡ ä¸­å„ªå…ˆç´š

**åŸå› **: æä¾›æ›´å‹å¥½å’Œçµæ§‹åŒ–çš„éŒ¯èª¤ä¿¡æ¯

**å»ºè­°å¯¦ç¾**:

```python
from fastapi.responses import JSONResponse
from ..inference.manager import (
    InferenceError, 
    ModelNotFoundError, 
    UnsupportedTaskError
)

# å®šç¾©éŒ¯èª¤ç¢¼
class ErrorCode:
    UNSUPPORTED_TASK = "UNSUPPORTED_TASK"
    MODEL_NOT_FOUND = "MODEL_NOT_FOUND"
    INVALID_INPUT = "INVALID_INPUT"
    INFERENCE_FAILED = "INFERENCE_FAILED"
    RESOURCE_EXHAUSTED = "RESOURCE_EXHAUSTED"
    TIMEOUT = "TIMEOUT"

# çµ±ä¸€éŒ¯èª¤éŸ¿æ‡‰æ¨¡å‹
class ErrorResponse(BaseModel):
    """çµ±ä¸€éŒ¯èª¤éŸ¿æ‡‰"""
    success: bool = False
    error: Dict[str, Any] = Field(
        description="éŒ¯èª¤è©³æƒ…",
        example={
            "type": "ModelNotFoundError",
            "message": "æ¨¡å‹æœªæ‰¾åˆ°",
            "code": "MODEL_NOT_FOUND",
            "details": {}
        }
    )
    timestamp: float

# æ·»åŠ ç•°å¸¸è™•ç†å™¨
@app.exception_handler(UnsupportedTaskError)
async def unsupported_task_exception_handler(request, exc):
    return JSONResponse(
        status_code=400,
        content={
            "success": False,
            "error": {
                "type": "UnsupportedTaskError",
                "message": str(exc),
                "code": ErrorCode.UNSUPPORTED_TASK,
                "details": {
                    "hint": "è«‹æª¢æŸ¥ä»»å‹™é¡å‹å’Œå¼•æ“çµ„åˆæ˜¯å¦æ”¯æŒ"
                }
            },
            "timestamp": time.time()
        }
    )

@app.exception_handler(ModelNotFoundError)
async def model_not_found_exception_handler(request, exc):
    return JSONResponse(
        status_code=404,
        content={
            "success": False,
            "error": {
                "type": "ModelNotFoundError",
                "message": str(exc),
                "code": ErrorCode.MODEL_NOT_FOUND,
                "details": {
                    "hint": "è«‹ç¢ºèªæ¨¡å‹å·²åœ¨ MLflow ä¸­è¨»å†Š"
                }
            },
            "timestamp": time.time()
        }
    )

# åœ¨ unified_inference ä¸­æ”¹é€²éŒ¯èª¤è™•ç†
@router.post("/infer", summary="çµ±ä¸€æ¨ç†æ¥å£")
def unified_inference(request: InferenceRequest):
    try:
        # ... ç¾æœ‰ä»£ç¢¼ ...
        
    except UnsupportedTaskError as e:
        logger.error(f"ä¸æ”¯æŒçš„ä»»å‹™: {e}")
        raise
    except ModelNotFoundError as e:
        logger.error(f"æ¨¡å‹æœªæ‰¾åˆ°: {e}")
        raise
    except ValueError as e:
        logger.error(f"åƒæ•¸éŒ¯èª¤: {e}")
        raise HTTPException(
            status_code=400,
            detail={
                "type": "ValueError",
                "message": str(e),
                "code": ErrorCode.INVALID_INPUT
            }
        )
    except Exception as e:
        logger.error(f"æ¨ç†åŸ·è¡Œå¤±æ•—: {e}")
        raise HTTPException(
            status_code=500,
            detail={
                "type": type(e).__name__,
                "message": str(e),
                "code": ErrorCode.INFERENCE_FAILED
            }
        )
```

**å½±éŸ¿**:
- âœ… æ›´æ¸…æ™°çš„éŒ¯èª¤ä¿¡æ¯
- âœ… æ›´å¥½çš„èª¿è©¦é«”é©—
- âœ… æ¨™æº–åŒ–çš„éŒ¯èª¤æ ¼å¼

---

## å¯¦æ–½è¨ˆåŠƒ

### Phase 1: ç«‹å³å¯¦æ–½ï¼ˆæœ¬å‘¨ï¼‰

```
â–¡ æ·»åŠ ç·©å­˜ç®¡ç†ç«¯é»
  â–¡ GET /cache/stats
  â–¡ GET /cache/models
  â–¡ DELETE /cache/models/{key}
  â–¡ PUT /cache/config

â–¡ æ·»åŠ æ‰¹æ¬¡æ¨ç†ç«¯é»
  â–¡ POST /batch

â–¡ æ¸¬è©¦æ–°ç«¯é»
```

### Phase 2: çŸ­æœŸå¯¦æ–½ï¼ˆæœ¬æœˆï¼‰

```
â–¡ æ”¹é€²éŒ¯èª¤è™•ç†
  â–¡ å®šç¾©éŒ¯èª¤ç¢¼
  â–¡ å¯¦ç¾ç•°å¸¸è™•ç†å™¨
  â–¡ æ›´æ–°ç¾æœ‰ç«¯é»

â–¡ å®Œå–„æ–‡æª”
  â–¡ æ›´æ–° API æ–‡æª”
  â–¡ æ·»åŠ ç¤ºä¾‹
```

### Phase 3: é•·æœŸè¦åŠƒï¼ˆä¸‹å­£åº¦ï¼‰

```
â–¡ ç•°æ­¥æ¨ç†æ”¯æŒ
â–¡ æ¨¡å‹é åŠ è¼‰
â–¡ WebSocket æµå¼æ¨ç†
â–¡ ç›£æ§å„€è¡¨æ¿
```

---

## æ¸¬è©¦å»ºè­°

### æ¸¬è©¦ç·©å­˜ç®¡ç†ç«¯é»

```bash
# 1. ç²å–ç·©å­˜çµ±è¨ˆ
curl http://localhost:8009/inference/cache/stats

# 2. æŸ¥çœ‹ç·©å­˜æ¨¡å‹
curl http://localhost:8009/inference/cache/models

# 3. ç§»é™¤ç‰¹å®šæ¨¡å‹
curl -X DELETE http://localhost:8009/inference/cache/models/ollama:llama2-7b

# 4. æ›´æ–°ç·©å­˜é…ç½®
curl -X PUT "http://localhost:8009/inference/cache/config?max_cache_size=10&max_memory_gb=16"
```

### æ¸¬è©¦æ‰¹æ¬¡æ¨ç†

```bash
curl -X POST http://localhost:8009/inference/batch \
  -H "Content-Type: application/json" \
  -d '{
    "requests": [
      {
        "task": "text-generation",
        "engine": "ollama",
        "model_name": "llama2-7b",
        "data": {"inputs": "Hello"},
        "options": {}
      },
      {
        "task": "text-generation",
        "engine": "ollama",
        "model_name": "llama2-7b",
        "data": {"inputs": "World"},
        "options": {}
      }
    ],
    "parallel": false
  }'
```

---

## çµè«–

**ç•¶å‰ç‹€æ…‹**: âœ… API èˆ‡ Inference æ¨¡çµ„å°é½Šè‰¯å¥½ï¼Œæ ¸å¿ƒåŠŸèƒ½å®Œæ•´

**å»ºè­°è¡Œå‹•**: 
1. ğŸ”´ æ·»åŠ ç·©å­˜ç®¡ç†ç«¯é»ï¼ˆé«˜å„ªå…ˆç´šï¼‰
2. ğŸ”´ æ·»åŠ æ‰¹æ¬¡æ¨ç†ï¼ˆé«˜å„ªå…ˆç´šï¼‰
3. ğŸŸ¡ æ”¹é€²éŒ¯èª¤è™•ç†ï¼ˆä¸­å„ªå…ˆç´šï¼‰

**é æœŸæ”¶ç›Š**:
- æ›´å¼·å¤§çš„ç·©å­˜ç®¡ç†èƒ½åŠ›
- æ›´é«˜çš„æ¨ç†ååé‡
- æ›´å¥½çš„é–‹ç™¼è€…é«”é©—

**é¢¨éšªè©•ä¼°**: âš ï¸ ä½é¢¨éšª
- æ‰€æœ‰å»ºè­°éƒ½æ˜¯æ–°å¢åŠŸèƒ½
- ä¸å½±éŸ¿ç¾æœ‰ API çš„ç©©å®šæ€§
- å‘ä¸‹å…¼å®¹

---

**å¯©æŸ¥è€…**: _____________  
**æ‰¹å‡†æ—¥æœŸ**: _____________  
**å¯¦æ–½è² è²¬äºº**: _____________
