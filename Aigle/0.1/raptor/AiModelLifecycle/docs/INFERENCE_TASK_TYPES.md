# Inference æ¨¡çµ„ä»»å‹™é¡å‹é…ç½®èªªæ˜

## ğŸ“‹ æ¦‚è¿°

ç‚ºäº†èˆ‡ `src/core/configs/inference.yaml` ä¸­çš„é…ç½®ä¿æŒä¸€è‡´ï¼Œinference æ¨¡çµ„ç¾å·²æ”¯æŒç´°åˆ†çš„ä»»å‹™é¡å‹ã€‚

## ğŸ”„ è®Šæ›´å…§å®¹

### åŸæœ‰è¨­è¨ˆ
```
text-generation â†’ æ”¯æŒ ollama å’Œ transformers
asr â†’ åƒ…æ”¯æŒ transformers
ocr â†’ åƒ…æ”¯æŒ transformers
```

### æ–°è¨­è¨ˆï¼ˆèˆ‡ core é…ç½®ä¸€è‡´ï¼‰
```
text-generation-ollama â†’ åƒ…æ”¯æŒ ollama
text-generation-hf â†’ åƒ…æ”¯æŒ transformers
text-generation â†’ é€šç”¨ï¼ˆå‘ä¸‹å…¼å®¹ï¼‰

asr-hf â†’ åƒ…æ”¯æŒ transformers
asr â†’ é€šç”¨ï¼ˆå‘ä¸‹å…¼å®¹ï¼‰

ocr-hf â†’ åƒ…æ”¯æŒ transformers  
ocr â†’ é€šç”¨ï¼ˆå‘ä¸‹å…¼å®¹ï¼‰

æ–°å¢: vad-hf, scene-detection, image-captioning, video-summary, audio-transcription
```

## ğŸ“Š å®Œæ•´ä»»å‹™é¡å‹åˆ—è¡¨

| ä»»å‹™é¡å‹ | æ”¯æŒçš„å¼•æ“ | MLflow Tag | æè¿° |
|---------|-----------|-----------|------|
| `text-generation-ollama` | ollama | `inference_task = 'text-generation-ollama'` | Ollama æ–‡æœ¬ç”Ÿæˆ |
| `text-generation-hf` | transformers | `inference_task = 'text-generation-hf'` | HuggingFace æ–‡æœ¬ç”Ÿæˆ |
| `text-generation` | ollama, transformers | - | é€šç”¨æ–‡æœ¬ç”Ÿæˆï¼ˆå‘ä¸‹å…¼å®¹ï¼‰ |
| `vlm` | transformers | `inference_task = 'vlm'` | è¦–è¦ºèªè¨€æ¨¡å‹ |
| `asr-hf` | transformers | `inference_task = 'asr-hf'` | HuggingFace èªéŸ³è­˜åˆ¥ |
| `asr` | transformers | `inference_task = 'asr'` | é€šç”¨èªéŸ³è­˜åˆ¥ |
| `vad-hf` | transformers | `inference_task = 'vad-hf'` | èªéŸ³æ´»å‹•æª¢æ¸¬ |
| `ocr-hf` | transformers | `inference_task = 'ocr-hf'` | HuggingFace OCR |
| `ocr` | transformers | `inference_task = 'ocr'` | é€šç”¨ OCR |
| `audio-classification` | transformers | `inference_task = 'audio_classification'` | éŸ³é »åˆ†é¡ |
| `video-analysis` | transformers | - | è¦–é »åˆ†æ |
| `scene-detection` | transformers | `inference_task = 'scene_detection'` | å ´æ™¯æª¢æ¸¬ |
| `document-analysis` | transformers | `inference_task = 'document_analysis'` | æ–‡æª”åˆ†æ |
| `image-captioning` | transformers | `inference_task = 'image_captioning'` | åœ–åƒæ¨™é¡Œç”Ÿæˆ |
| `video-summary` | transformers | `inference_task = 'video_summary'` | è¦–é »æ‘˜è¦ |
| `audio-transcription` | transformers | `inference_task = 'audio_transcription'` | éŸ³é »è½‰éŒ„ |

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### 1. ä½¿ç”¨ Ollama å¼•æ“ï¼ˆæ¨è–¦ä½¿ç”¨ç´°åˆ†ä»»å‹™é¡å‹ï¼‰

```python
# æ–°æ–¹å¼ï¼ˆæ¨è–¦ï¼‰
response = requests.post(
    "http://localhost:8000/inference/infer",
    json={
        "task": "text-generation-ollama",  # æ˜ç¢ºæŒ‡å®š Ollama ä»»å‹™
        "engine": "ollama",
        "model_name": "llama2-7b-chat",
        "data": {"inputs": "ä½ å¥½"},
        "options": {"max_length": 100}
    }
)

# èˆŠæ–¹å¼ï¼ˆå‘ä¸‹å…¼å®¹ï¼‰
response = requests.post(
    "http://localhost:8000/inference/infer",
    json={
        "task": "text-generation",  # é€šç”¨ä»»å‹™é¡å‹
        "engine": "ollama",
        "model_name": "llama2-7b-chat",
        "data": {"inputs": "ä½ å¥½"},
        "options": {"max_length": 100}
    }
)
```

### 2. ä½¿ç”¨ HuggingFace å¼•æ“

```python
# æ–°æ–¹å¼ï¼ˆæ¨è–¦ï¼‰
response = requests.post(
    "http://localhost:8000/inference/infer",
    json={
        "task": "text-generation-hf",  # æ˜ç¢ºæŒ‡å®š HF ä»»å‹™
        "engine": "transformers",
        "model_name": "gpt2",
        "data": {"inputs": "äººå·¥æ™ºèƒ½"},
        "options": {"max_length": 50}
    }
)

# ASR ä»»å‹™
response = requests.post(
    "http://localhost:8000/inference/infer",
    json={
        "task": "asr-hf",  # æ˜ç¢ºæŒ‡å®š HF ASR
        "engine": "transformers",
        "model_name": "whisper-large",
        "data": {"audio": "/path/to/audio.wav"},
        "options": {}
    }
)
```

## ğŸ”§ é…ç½®æ–‡ä»¶å°æ‡‰

### inference.yaml é…ç½®
```yaml
task_to_models:
  text-generation-ollama:
    strategy: "priority"
    discovery:
      source: "mlflow"
      filter: "tags.inference_task = 'text-generation-ollama'"
  
  text-generation-hf:
    strategy: "priority"
    discovery:
      source: "mlflow"
      filter: "tags.inference_task = 'text-generation-hf'"
```

### MLflow æ¨¡å‹è¨»å†Š
```python
from mlflow.tracking import MlflowClient

client = MlflowClient()

# Ollama æ¨¡å‹
client.set_model_version_tag(
    name="llama2-7b-chat",
    version="1",
    key="inference_task",
    value="text-generation-ollama"  # å°æ‡‰é…ç½®æ–‡ä»¶
)

client.set_model_version_tag(
    name="llama2-7b-chat",
    version="1",
    key="ollama_model_name",
    value="llama2:7b"  # Ollama æœ¬åœ°åç¨±
)

# HuggingFace æ¨¡å‹
client.set_model_version_tag(
    name="gpt2-chinese",
    version="1",
    key="inference_task",
    value="text-generation-hf"  # å°æ‡‰é…ç½®æ–‡ä»¶
)
```

## ğŸ“ é·ç§»æŒ‡å—

### å¾èˆŠ API é·ç§»åˆ°æ–° API

#### æ–‡æœ¬ç”Ÿæˆä»»å‹™

**èˆŠæ–¹å¼:**
```python
{
    "task": "text-generation",
    "engine": "ollama"  # éœ€è¦æ˜ç¢ºæŒ‡å®šå¼•æ“
}
```

**æ–°æ–¹å¼ï¼ˆæ¨è–¦ï¼‰:**
```python
{
    "task": "text-generation-ollama",  # ä»»å‹™é¡å‹å·²åŒ…å«å¼•æ“ä¿¡æ¯
    "engine": "ollama"
}
```

**å‘ä¸‹å…¼å®¹:**
- èˆŠçš„ `text-generation` ä»»å‹™é¡å‹ä»ç„¶æ”¯æŒ
- ç³»çµ±æœƒæ ¹æ“š `engine` åƒæ•¸è‡ªå‹•è·¯ç”±

#### èªéŸ³è­˜åˆ¥ä»»å‹™

**èˆŠæ–¹å¼:**
```python
{
    "task": "asr",
    "engine": "transformers"
}
```

**æ–°æ–¹å¼ï¼ˆæ¨è–¦ï¼‰:**
```python
{
    "task": "asr-hf",  # æ˜ç¢ºæ¨™è¨»ç‚º HuggingFace
    "engine": "transformers"
}
```

## âš ï¸ æ³¨æ„äº‹é …

1. **ä»»å‹™é¡å‹å‘½åè¦å‰‡**
   - `-ollama` å¾Œç¶´ï¼šå°ˆç”¨æ–¼ Ollama å¼•æ“
   - `-hf` å¾Œç¶´ï¼šå°ˆç”¨æ–¼ HuggingFace/Transformers å¼•æ“
   - ç„¡å¾Œç¶´ï¼šé€šç”¨é¡å‹ï¼ˆå‘ä¸‹å…¼å®¹ï¼‰

2. **MLflow Tag å°æ‡‰**
   - æ¨¡å‹åœ¨ MLflow ä¸­çš„ `inference_task` tag æ‡‰èˆ‡ä»»å‹™é¡å‹åç¨±ä¸€è‡´
   - Ollama æ¨¡å‹éœ€é¡å¤–æ·»åŠ  `ollama_model_name` tag

3. **å‘ä¸‹å…¼å®¹æ€§**
   - æ‰€æœ‰èˆŠçš„ä»»å‹™é¡å‹åç¨±ä»ç„¶æ”¯æŒ
   - å»ºè­°æ–°ä»£ç¢¼ä½¿ç”¨ç´°åˆ†çš„ä»»å‹™é¡å‹

4. **å¼•æ“é©—è­‰**
   - ç³»çµ±æœƒé©—è­‰ä»»å‹™é¡å‹èˆ‡å¼•æ“çš„å…¼å®¹æ€§
   - ä¾‹å¦‚ï¼š`text-generation-ollama` åªèƒ½ä½¿ç”¨ `ollama` å¼•æ“

## ğŸ§ª æ¸¬è©¦

### æ¸¬è©¦è…³æœ¬

```bash
# æ¸¬è©¦ Ollama æ¨ç†
python test/test_ollama_mlflow_mapping.py

# æ¸¬è©¦ä¸åŒä»»å‹™é¡å‹
python test/test_task_types.py  # éœ€è¦å‰µå»º
```

### API æ¸¬è©¦

```python
import requests

# æ¸¬è©¦æ‰€æœ‰æ”¯æŒçš„ä»»å‹™é¡å‹
tasks = [
    ("text-generation-ollama", "ollama", "llama2-7b"),
    ("text-generation-hf", "transformers", "gpt2"),
    ("asr-hf", "transformers", "whisper-large"),
    ("ocr-hf", "transformers", "trocr-base"),
]

for task, engine, model in tasks:
    response = requests.post(
        "http://localhost:8000/inference/infer",
        json={
            "task": task,
            "engine": engine,
            "model_name": model,
            "data": {"inputs": "æ¸¬è©¦"},
            "options": {}
        }
    )
    print(f"{task}: {response.status_code}")
```

## ğŸ“š ç›¸é—œæ–‡æª”

- [inference.yaml](../src/core/configs/inference.yaml) - Core é…ç½®æ–‡ä»¶
- [OLLAMA_FIX_SUMMARY.md](./OLLAMA_FIX_SUMMARY.md) - Ollama ä¿®å¾©èªªæ˜
- [router.py](../src/inference/router.py) - ä»»å‹™è·¯ç”±å¯¦ç¾
- [manager.py](../src/inference/manager.py) - æ¨ç†ç®¡ç†å™¨

## ğŸ”„ ç‰ˆæœ¬ä¿¡æ¯

- **ç‰ˆæœ¬**: v2.1.0
- **æ›´æ–°æ—¥æœŸ**: 2025-10-13
- **è®Šæ›´é¡å‹**: åŠŸèƒ½å¢å¼·ï¼Œä¿æŒå‘ä¸‹å…¼å®¹
- **å½±éŸ¿ç¯„åœ**: ä»»å‹™é¡å‹å®šç¾©ã€è·¯ç”±é‚è¼¯ã€API æ–‡æª”
